---
title: "Introduction to Spatial Data & Using R as a GIS"
output: pdf_document
author: "Nick Bearman - Clear Mapping Co"
---

| Learning Outcomes:  | R Functions & Libraries: |
| -- | -- |
| Use R to read in CSV data |  `read.csv()` (pg. 4)  | 
| Use R to read in spatial data | `readOGR()` (pg. 6)  | 
| Know how to plot spatial data using R | `plot()` (pg. 6)  | 
| Customize colour & classifications  | `classIntervals()` (pg. 11)  | 
| Understand how to use loops for multiple maps  | `for(){}` (pg. 13)  | 
| Know how to reproject spatial data | `spTransform()` (pg. 15)  | 
| Be able to perform point in polygon  | `gContains()` (pg. 17)  | 
| Know how to write shape files | `writeOGR()` (pg. 18)  | 
| Know how create a 'heat-map'  |  `spatstat` librar (pg. 19)  | 
<!-- Tables have a max line length, if it is too long, the table gets shrunk -->

## R Basics

R began as a statistics program, and is still used as one by many users. At a simple level you can type in "3 + 4", press return, and R will respond "7". The code you type in in this tutorial is shown like this:

```{r, comment=NA, eval=FALSE}
3 + 4
```

And R's output is shown like this: 

```{r, echo=FALSE, comment=NA}
7
```

R has developed into a GIS as a result of user contributed packages, or libraries, as R refers to them. We will be using several libraries in this practical, and will load them as necessary. If you are using your personal computer, you will need to install the R libraries, as well as loading them. To do this, run `install.packages("package_name")`.

We won't spend too much time on the very basics of using R - if you want to find out more, there are some good tutorials at http://www.social-statistics.org/?p=764 or http://rpubs.com/nickbearman/gettingstartedwithr. 

We are going to use a program called [RStudio](http://www.rstudio.com/ "R Studio website"), which works on top of R and provides a good user interface. I'll talk a little bit about it in the presentation, but the key areas of the window are these:

![Screenshot of RStudio](materials/images/rstudio-labelled.png)

Open up R Studio (click **Start > All Programs > RStudio > RStudio** or double-click the icon on the desktop). 

R can initially be used as a calculator - enter the following into the left hand side of the window - the section labelled **Console**:
```{r,eval=FALSE}
6 + 8
```

Don't worry about the `[1]` for the moment - just note that R printed out `14` since this is the answer to the sum you typed in.  In these worksheets, sometimes I show the results of what you have typed in.  This is in the format shown below:
```{r, comment=NA}
5 * 4
```
Also note that `*` is the symbol for multiplication here - the last command asked R to perform the calculation '5 times 4'.  Other symbols are `-` for subtraction and `/` for division:
```{r, comment=NA}
12 - 14
6 / 17
```


You can also assign the answers of the calculations to variables,  and use them in calculations.  You do this as below
```{r, comment=NA}
price <- 300
```
Here,  the value `300` is stored in the variable `price`.  The `<-` symbol means put the value on the right into the variable on the left, it is typed with a `<` followed by a `-`. The variables are shown in the window labelled 'Environment', in the top right. Variables can be used in subsequent calculations.  For example, to apply a 20% discount to this price,  you could enter the following:
```{r, comment=NA}
price - price * 0.2 
```

or use intermediate variables:
```{r,tidy=FALSE, comment=NA}
discount <- price * 0.2
price - discount
```

R can also work with lists of numbers,  as well as individual ones. Lists are specified using the `c` function.  Suppose you have a list of house prices listed in an estate agents,  specified in thousands of pounds.  You could store them in a variable called `house.prices` like this:

```{r, comment=NA}
house.prices <- c(120,150,212,99,199,299,159)
house.prices
```
Note that there is no problem with full stops in the middle of variable names.

You can then apply functions to the lists.  For example to take the average of a list,  enter:
```{r, comment=NA}
mean(house.prices)
```
If the house prices are in thousands of pounds,  then this tells us that the mean house price is £176,900.  Note that on your display,  the answer may be displayed to more significant digits,  so you may have something like `r mean(house.prices)` as the mean value.

The Data Frame
------------
R has a way of storing data in an object called a **data frame**. This is rather like an internal spreadsheet where all of the relevant data items are stored together as a set of columns.  This is similar to the data set storage in Excel or SPSS where each variable corresponds to a column and each case (or observation) corresponds to a row. However, while SPSS can only have one data set active at a time, in R you can have several of them.

We have a CSV file of house prices and burglary rates, which we can load into R. We can use a function called `read.csv` which, as you might guess, reads CSV files. Run the line of code below, which loads the CSV file into a variable called `hp.data`. 

<!-- There are various data files used for this workshop. Ideally I would pull them in directly from GitHub with code, but I can't work out how to do this. A copy of all the data used is in the data sub-directory. Using https://raw.githubusercontent.com/nickbearman/intro-r-spatial-analysis/master/data/hpdata.csv just gives an error message. -->
<!-- Update hp.data <- read.csv("https://raw.githubusercontent.com/nickbearman/intro-r-spatial-analysis/master/data/hpdata.csv") does work but the URL is much longer, which is annoying -->

```{r getdata}
hp.data <- read.csv("http://nickbearman.me.uk/data/r/hpdata.csv")
```

When we read in data, it is always a good idea to check it came in ok. To do this, we can preview the data set. The `head` command shows the first 6 rows of the data.

```{r, comment=NA}
head(hp.data)
```

You can also click on the variable listed in the Environment window, which will show the data in a new tab. You can also enter:

```{r, comment=NA}
View(hp.data)
```
to open a new tab showing the data. 

You can also describe each column in the data set using the `summary` function:
```{r, comment=NA, eval = FALSE}
summary(hp.data)
```
For each column,  a number of values are listed:

Item    | Description
--------|----------------------------------------------------------------------------
Min.    | The smallest value in the column
1st. Qu.| The first quartile (the value 1/4 of the way along a sorted list of values)
Median  | The median (the value 1/2 of the way along a sorted list of values)
Mean    | The average of the column
3rd. Qu.| The third quartile (the value 3/4 of the way along a sorted list of values)
Max.    | The largest value in the column

<!-- Ideally want to add the paragraph below into a 'Notes on Summary Data' box -->
*Based on these numbers, an impression of the spread of values of each variable can be obtained. In particular it is possible to see that the median house price in St. Helens by neighbourhood ranges from £65,000 to £260,000 and that half of the prices lie between £152,500 and £210,000.  Also it can be seen that since the median measured burglary rate is zero, then at least half of areas had no burglaries in the month when counts were compiled.*

We can use square brackets to look at specific sections of the data frame, for example `hp.data[1,]` or `hp.data[,1]`. We can also delete columns and create new columns using the code below. Remember to use the `head()` command as we did earlier to look at the data frame. 

```{r, comment=NA}
#create a new column in hp.data dataframe call counciltax, storing the value NA
hp.data$counciltax <- NA
#see what has happened
head(hp.data)

#delete a column
hp.data$counciltax <- NULL
#see what has happened
head(hp.data)

#rename a column
colnames(hp.data)[1] <- "IDNumber"
#see what has happened
head(hp.data)
```

Now is a good time to remind you to save your data on a regular basis. This is particularly important if you are working on a project, and need to reload your data later on. R has a number of different elements you can save. The workspace is the most important element, as it contains any data frames or other objects you have created; i.e. everything listed in the 'Environment' tab, like the `hp.data` object we created earlier. To do this, click the save button in the Environment tab. Choose somewhere to save it (your Documents folder is a good place) and give it a name. To load these in a new session, click File > Open File and select your file.

Geographical Information
-------------------------

Until now,  although this is geographical data, no maps have been drawn. Firstly, you need to load some new *packages* into R - a package is an extra set of functionality that extends what R is capable of doing.  Here, one of the new package is called `maptools` and it extends R by allowing it to draw maps, and handle geographical information.  The packages are already installed, so all you need to do is to let R know you want to use the packages:

```{r,message=FALSE,display=FALSE, warning = FALSE}
library(maptools)
library(rgdal)
library(classInt)
library(RColorBrewer)
```

*Remember: if you are using your own laptop, you will need to install the packages - see the note on the front page. Ask for help if you are unsure.*

However, this just makes R *able* to handle geographical data, it doesn't actually load any specific data sets.  To do this,  you'll need to obtain them from somewhere. Data for maps in R can be read in from **shapefiles** - these are a well known GIS data format. The first task is to obtain a shapefile set of the St. Helens neighbourhoods in Merseyside (or Lower layer Super Output Areas - LSOA, as they are more formally called).  To do this,  you will need to put some information into a working folder.  

R uses working folders to store information relevant to the current project you are working on. I suggest you make a folder called **R work** in the M: drive (using Computer in the Start Menu). Then we need to tell R where this is, so click **Session > Set Working Directory > Choose Directory...** and selecting the folder that you created. 

As with most programs, there are multiple ways to do things. For instance, to set the working directory we could type: `setwd("M:/R work")`. Your version might well have a longer title, depending on what you called the folder. Also note that slashes are indicated with a '*/*' not '\\'. 

There is a set of shapefiles for the St. Helens neighbourhoods at the same location as the data set you read in earlier. Since several files are needed,  I have bundled these together in a single zip file.  You will now download this to your local folder and subsequently unzip it. This can all be done via R functions:

```{r, comment=NA}
download.file("http://www.nickbearman.me.uk/data/r/sthelens.zip","sthelens.zip")
unzip("sthelens.zip")
```

The first function actually downloads the zip file into your working folder,  the second one unzips it,  creating the shapefile set. All of the shapefile set files begin with `sthelens` but then have different endings,  e.g. `sthelens.shp`, `sthelens.dbx` and `sthelens.shx`.
Now, we can read these into R. 

```{r, comment=NA, message=FALSE, results='hide'}
sthelens <- readOGR(".","sthelens")
```
The `readOGR` function does this,  and stores them into another type of object called a `SpatialPolygonsDataFrame` object.  These polygons are areas or regions,  such as the neighbourhoods (LSOA) in St. Helens.  You can use the `plot` function to draw the polygons (i.e. the map of the LSOA).

```{r, comment=NA}
plot(sthelens)
```
We can also look at the data for each LSOA (in R this is known as the `data` slot, and is the same as the attribute table in programs like ArcGIS, QGIS or MapInfo. If you want to open the shapefile in QGIS or ArcGIS to have a look, feel free to).

```{r, eval=F,comment=NA}
head(sthelens@data)
```
  
  
  
```{r, echo=F,comment=NA}
head(sthelens@data[,1:7])
```

You can see there is a lot of information there, but the useful bit is the `SP_ID` field. You may have spotted that these IDs match those in the `hp.data` file - we can use this to join the two data sets together, and then show the Burglary rates (from `hp.data`) on the map. The idea is that there is a field in each data set that we can use to join the two together; in this case we have the `ID` field in `sthelens` and the `SP_ID` field in `hp.data`. 

```{r, comment=NA}
sthelens.prices <- sthelens
sthelens.prices@data <- merge(sthelens@data,hp.data,by.x="SP_ID",by.y="IDNumber", all.x=TRUE)
```
 
And use the `head` function to check the data have been joined correctly. 

```{r,comment=NA}
head(sthelens.prices@data)
```
 
Now that we have joined the data together, we can draw a choropleth map of these house prices.  In later exercises more will be explained as to how this works, but for now the method will simply be demonstrated.  

```{r, eval = F, comment=NA}
var <- sthelens.prices@data[,"Burglary"]
breaks <- classIntervals(var, n = 6, style = "fisher")
my_colours <- brewer.pal(6, "Greens")
plot(sthelens.prices, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)],
     axes = FALSE, border = rgb(0.8,0.8,0.8))
```

```{r, echo=F, comment=NA}
var <- sthelens.prices@data[,"Burglary"]
breaks <- classIntervals(var, n = 6, style = "fisher")
my_colours <- brewer.pal(6, "Greens")
plot(sthelens.prices, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)],
     axes = FALSE, border = rgb(0.8,0.8,0.8))
```

For now, just type this into R and hit return. Don't worry about trying to interpret it, we will look at this in more detail later.  Note that it is important to make sure the upper and lower case letters you type in are **exactly** the same as the ones above.  This is generally the case with R.  

The map shows the different burglary rates,  with dark shading indicating a higher rate. However, this is much easier to interpret if you add a legend to the map.  For now,  just enter the following code. As before,  more about how all of the commands work will be disclosed later in the course.

```{r,eval=FALSE}
legend(x = 357000, y = 393000, legend = leglabs(breaks$brks), fill = my_colours, bty = "n", 
       cex = 0.8)
```

You can also add a title to the map:
```{r,eval=FALSE}
title('Burglary Rates per 10,000 Homes in St. Helens')
```
As a final useful technique you can copy and paste maps like this into Word documents.  Click on the Export button, and then choose Copy to Clipboard.... Then choose Copy Plot. If you also have Word up and running, you can then paste the map into your document.

##Making a Map with Census Data

You have seen how we can use R to make maps, and this often involves several lines of code. Using a script can help with this - they allow you to run a  series of commands in sequence, and be able to update one or two lines without having to retype all of the code. Create a new script (**File > New File > R Script**) and enter the code in there. Then you can select the lines you want to run by highlighting them, and then pressing `Ctrl+Enter`, or using the **Run** button. Use a script for all the R work you do from now on - it is much easier to correct mistakes and re-run than editing commands in the console. 

Now we are going to use the same principle as we used before to create a map of some data from the 2011 Census. We need to download the data, and although there are other sources of these data, in this example we will use the https://www.nomisweb.co.uk/ website.

- Navigate to https://www.nomisweb.co.uk/. 
- Under Census Statistics > 2011 Census click "Data catalogue". 
- Select "Key Statistics (KS)". 
- Choose "KS102EW" (Age Structure). 
- There are various different ways of downloading the data - explore the different options. We are going to download the table as a CSV file. 
- Under Download (.csv) choose "super output areas - lower layer 2011" from the drop down list. 
- Click "Download". 
- A file called "bulk.csv" will be downloaded - save this in your working directory.  

<!-- Last checked 20170622 -->

Open this file up in Excel, and you can see there are a number of different columns, covering different data. We are interested in the age data - scroll across and see the different values we have. 

Add the command below to your script, and run it to read in the CSV file. The `header = TRUE` tells R to assign the first row as variable names. 

```{r, echo=FALSE, comment=NA}
#download csv file
  download.file("http://www.nickbearman.me.uk/data/r/nomis-2011-age-data.zip","nomis-2011-age-data.zip")
#unzip csv file
  unzip("nomis-2011-age-data.zip")
```

```{r, comment=NA}
pop2011 <- read.csv("bulk.csv", header = TRUE)
```

Then run `head` to see that the data has been read in correctly. *R will show all 45 variables, where as I've only shown the first 7 in the handout*. 

```{r, eval=F}
head(pop2011)
```

```{r, echo=F,comment=NA}
head(pop2011[,1:7])
```

You can see that some of the variable names are not displayed very clearly. We can rename the columns, so that when we run the `head` command, R lists the correct names. This will also help us refer to the columns later on. Run the code below, which creates a new variable which contains the names (`newcolnames`) and then applies it to the `pop2011` data frame. 

It's also worth noting here that any line of code that starts with a `#` is a comment - i.e. R will ignore that line and move onto the next. I've included them here so you can see what is going on, but you don't need to type them in.  
  
```{r, comment=NA}
#create a new variable which contains the new variable names
  newcolnames <- c("AllUsualResidentsc","Age00to04","Age05to07",
                   "Age08to09","Age10to14","Age15","Age16to17",
                   "Age18to19","Age20to24","Age25to29",
                   "Age30to44","Age45to59","Age60to64",
                   "Age65to74","Age75to84","Age85to89",
                   "Age90andOver","MeanAge","MedianAge")

#apply these to pop2011 data frame
  colnames(pop2011)[5:23] <- newcolnames
```

The final line of code (starting `colnames`) actually updates the variable names. The square brackets are used to refer to specific elements- in this case, columns 5 to 23. *For example, `pop2011[1,]` will show the first row and `pop2011[,1]` will show the first column.*

Now we have the correct column names for the data frame. It would also be good to check they have been applied to the `pop2011` dataframe correctly. 

Now we have the attribute data (the number of people in each age group in each LSOA in this case) we need to join this attribute data to the spatial data. Therefore, first, we need to download the spatial data. 

- Go to http://census.edina.ac.uk/ and select Boundary Data Selector. 
- Then set Country to England, Geography to Statistical Building Block, dates to 2011 and later, and click 'Find'.
- Select 'English Lower Layer Super Output Areas, 2011' and click 'List Areas'. 
- Select 'Liverpool' from the list and click 'Extract Boundary Data'. 
- Click 'Go to Bookmark Facility' and you should be able to download the data after a 10 to 30 second wait. 

<!-- Last checked 20170411 -->

Extract the files, and move all the files starting with the name `england_lsoa_2011Polygon` to your working folder. Then read in the data:

```{r, echo=FALSE, comment=NA}
download.file("http://www.nickbearman.me.uk/data/r/england_lsoa_2011.zip","england_lsoa_2011.zip")
unzip("england_lsoa_2011.zip")
```

```{r, comment=NA, results='hide'}
#read in shapefile
LSOA <- readOGR(".", "england_lsoa_2011")
```

Like earlier, we can use the `plot` command to preview the data. Try `plot(LSOA)`. We can also apply the same technique with `head()` to the attribute table. Remember we need to use `head(LSOA@data)` because this is a spatial data frame. 

The next stage is to join the attribute data to the spatial data, like we did in the exercise earlier. See if you can see how I have changed the code from earlier. 

```{r, comment=NA}
#join attribute data to LSOA
LSOA@data <- merge(pop2011,LSOA@data,by.x="geography.code",by.y="code")
```

And use the `head` command to check it has joined correctly. Your data should contain the LSOA_CODE in the first column, and correctly labelled Age data in the 11th to 45th columns. 

```{r, eval=F}
head(LSOA@data)
```

```{r, eval=F,echo=F,comment=NA}
head(LSOA@data[,1:6])
```

##Making Maps

Now we have all the data setup, we can actually create the map. Run the code below, which is the same as the code we used earlier. Remember to use a script to run this code, like we did before. Ask for help if you are not sure. 

<!-- Code for students in R Studio -->
```{r, eval=F,comment=NA}
#select variable
var <- LSOA@data[,"Age00to04"]
#set colours & breaks
breaks <- classIntervals(var, n = 6, style = "fisher")
my_colours <- brewer.pal(6, "Greens")
#plot map
plot(LSOA, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)], axes = FALSE, 
     border = rgb(0.8,0.8,0.8))
#draw legend
legend(x = 328130, y = 386506.5, legend = leglabs(breaks$brks), fill = my_colours, bty = "n")
```

<!-- Code for correct PDF rendering -->
```{r, echo=F,comment=NA}
#select variable
var <- LSOA@data[,"Age00to04"]
#set colours & breaks
breaks <- classIntervals(var, n = 6, style = "fisher")
my_colours <- brewer.pal(6, "Greens")
#plot map
plot(LSOA, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)], axes = FALSE, 
     border = rgb(0.8,0.8,0.8))
#draw legend
legend(x = 328030, y = 387506.5, legend = leglabs(breaks$brks), fill = my_colours, bty = "n", cex = 0.7)
```

This code draws the map, and adds a legend. Due to the way R works, we have to build up the map in stages, and each line of code adds a different element to the map. With blocks of code like this (and in fact, with all code) it is important to add comments, so we can remind ourselves what the different bit of code do. Comments in R are preceded with a #, as you can see above. 

The first line tells R which variable to plot (`#select variable`). Try altering the first line to `var <- LSOA@data[,"Age05to07"]` and run the code again. Try generating maps for different variables (remember you can use `colnames(LSOA@data)` to list the variables).

You can also add a title to the map, which is useful to say what variable you are mapping. Add this code to your script and run it. 

```{r,eval=FALSE}
title('Count of Population ages 0 to 4 in Liverpool')
```

Remember that any line starting with a `#` is a comment, and will be ignored by R. Comments are very useful for us to note what the code is doing, particularly when you come back to it 6 months later and can't remember what it is supposed to do!

##Colours and Categories (optional exercise)

The second section of the code (`#set colours & breaks`) tells R how many categories to split the data into (`n = 6`) and which classification method to use (`style = "fisher"`). Try altering the number of categories and classification method. Type `?classIntervals` into the console to see the help file on this function, and explore the different classification options. Remember we can use `hist(LSOA@data$Age00to04pc)` to view the histogram for a specific data set. See also if you can change the colours - explore the help file for details and try running `display.brewer.all()`. 

If you want to set the breaks manually, this can be done using the `fixed` style: `breaks <- classIntervals(var, n = 6, style = "fixed", fixedBreaks=c(0, 50, 100, 150, 200, 250))`. See the example in the help file for more details.

You can also show a histogram with the classification breaks using this code:

```{r,eval=FALSE}
#select the variable
var <- LSOA@data[,"Age00to04"]
#calculate the breaks
breaks <- classIntervals(var, n = 6, style = "fisher")
#draw histogram
hist(var)
#add breaks to histogram
abline(v = breaks$brks, col = "red")
```

*R can be expanded to create many different sorts of maps. Proportional Symbols are possible (see this article http://www.jstatsoft.org/v15/i05/paper and library http://cran.r-project.org/web/packages/rCarto/rCarto.pdf). Another library that you might come across is `ggplot` which develops the standard `plot` function that we have been using, both for non-spatial and spatial data.*

##Scale Bar and North Arrow (optional exercise)

It is also good practice to add a scale bar and a north arrow to each of the maps you produce. Running this code will add these to the map:

```{r,eval=FALSE}
#add north arrow
SpatialPolygonsRescale(layout.north.arrow(2), offset= c(336030,382006), scale = 2000, 
                       plot.grid=F)
#add scale bar
SpatialPolygonsRescale(layout.scale.bar(), offset= c(328030,381506), scale= 5000, 
                       fill= c("white", "black"), plot.grid= F)
#add text to scale bar 
text(328030,381006,"0km", cex=.6)
text(328030 + 2500,381006,"2.5km", cex=.6)
text(328030 + 5000,381006,"5km", cex=.6)
```

Remember to adjust the position of these items if you need to. To help with this, there is the `locator()` function. In RStudio, type `locator(1)` into the console and press enter. When you move your mouse over the plots window (where the maps are) the mouse pointer will turn into a cross. If you click somewhere on the plot, RStudio will give you the coordinates of the point you selected. You can then substitute these into the code to locate the legend / scale bar etc. wherever you wish. 

It might also be good to have a 'N' under the North arrow to explain what it means. Look at the parameter `cex` in the helpsheet - it allows you to resize items. Try applying this with the `text` function to add the 'N' in the appropriate place in a suitable size. 

##Exporting and Multiple Maps (optional exercise)

One way of saving the map is using the Export option in the plot window. We can also do this using code, by adding two lines: `pdf(file="image.pdf")` before the map code and `dev.off()` after the map code. Try it now, and R will save the map in your working directory. 

```{r, comment=NA, eval=FALSE}
#output to PDF
pdf(file="image.pdf")
  var <- LSOA@data[,"Age00to04"]
  breaks <- classIntervals(var, n = 6, style = "fisher")
  my_colours <- brewer.pal(6, "Greens")
  plot(LSOA, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)],   
       axes = FALSE, border = rgb(0.8,0.8,0.8))
  legend(x = 330130, y = 385506.5, legend = leglabs(breaks$brks), fill = my_colours, bty = "n")
  title('Percentage of Population ages 0 to 4 in Liverpool')
  #add north arrow
  SpatialPolygonsRescale(layout.north.arrow(2), offset= c(336030,382006), scale = 2000, 
                         plot.grid=F)
  #add scale bar
  SpatialPolygonsRescale(layout.scale.bar(), offset= c(328030,381506), scale= 5000, 
                       fill= c("white", "black"), plot.grid= F)
  #add text to scale bar 
  text(331030,379206,"0km", cex=.6)
  text(331030 + 2500,379206,"2.5km", cex=.6)
  text(331030 + 5000,379206,"5km", cex=.6)
#stop PDF outout
dev.off()
```

Saving the map using code allows us to create multiple maps very easily. A variable (`mapvariables`) is used to list which variables should be mapped (using the column numbers), and then the line starting `for` starts a loop. Try running the code, and then change the variables it maps (remember `colnames(LSOA@data)` will be useful).

```{r, comment=NA}
#setup variable with list of maps to print
mapvariables <- c(6,7,8)
#loop through for each map
for (i in 1:length(mapvariables)) {
  #setup file name
  filename <- paste0("map",colnames(LSOA@data)[mapvariables[i]],".pdf")
  #output to PDF
  pdf(file=filename)
  #create map
  var <- LSOA@data[,mapvariables[i]]
  #colours and breaks
  breaks <- classIntervals(var, n = 6, style = "fisher")
  my_colours <- brewer.pal(6, "Greens")
  #plot map
  plot(LSOA, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)], 
       axes = FALSE, border = rgb(0.8,0.8,0.8))
  #add legend
  legend(x = 330130, y = 384506.5, legend = leglabs(breaks$brks), fill = my_colours, bty = "n")
  #add title
  title(colnames(LSOA@data)[mapvariables[i]])
  #Add North Arrow
  SpatialPolygonsRescale(layout.north.arrow(2), offset= c(336030,382006), scale = 2000, 
                         plot.grid=F)
  #add Scale Bar
  SpatialPolygonsRescale(layout.scale.bar(), offset= c(328030,381506), scale= 5000, 
                       fill= c("white", "black"), plot.grid= F)
  #add text to scale bar 
  text(331030,379206,"0km", cex=.6)
  text(331030 + 2500,379206,"2.5km", cex=.6)
  text(331030 + 5000,379206,"5km", cex=.6)
  #stop PDF output
  dev.off()
}
```

##Creating a Tidy Map Title (optional exercise)

As you may have spotted, the map title isn't great. Just using the field name does give us the information we require, but it doesn't make a good looking map. 

We can construct the title in R, based on the field name. Using a function called `substr()` we can extract the years we need from the field name, and print a more sensible title. 

We want a title like **Percentage population Aged 0 to 4** with the numbers replaced according to the current variable. We can use the `substr()` function for this. For example:

```{r, comment=NA, eval = FALSE}
#setup example text string which is the column name
  textstring <- "Age00to04"
#use substr to extract the 4th to 5th and 8th to 9th characters
  firstyear <- substr(textstring, 4, 5)
  lastyear <- substr(textstring, 8, 9)
#create title
  paste0("Percentage population Aged ", firstyear, " to ", lastyear)
```
Try incorporating this into your map script. There is a complete version (script-multiple-maps-title.R) but try not to look at it straight away!

An additional point - what happens for Age 15? How could we alter the code (or field names) to take this into account? And how could we handle ages with only one digit in a better way?

##Clustering of Crime Points 

In this section we will look at a couple of different types of GIS analysis, using some crime data for Liverpool. These are just a couple of the analysis techniques that are available, and the aim is to teach you the techniques you need to know. Then you can apply these techniques to other data sets and other analytical methods in R.

RStudio allows you to have projects, where it will tie together all of the different elements of a piece of R work (scripts, history, workspace etc.). Try starting a new project for this next section. It will ask you if you want to save your existing work - choose yes. It will also ask whether you want to create a new, empty project (you do) and where you want to save it (create a new working directory for this). 

We need to read in the crime data, and because the crime data is just a CSV file, we need to do some processing in order to get it as spatial data in R, including some reprojecting of the data (converting the coordinate system from WGS 1984 to BNG).

```{r, comment=NA,warning=FALSE,message=FALSE}
#load library
library(maptools)
#Read the data into a variable called crimes
crimes <- read.csv("http://nickbearman.me.uk/data/r/police-uk-2017-01-merseyside-street.csv")
```

Now the CSV data has been read in, take a quick look at it using `head(crimes)`. You will see that the data consists of a number of columns, each with a heading. Two of these are called *Longitude* and *Latitude* – these are the column headers that give the coordinates of each incident in the data you have just downloaded. Another is headed *Crime.Type* and tells you which type of crime occurred. 

At the moment, the data is just in a data frame object - not any kind of spatial object.  To create the spatial object,  enter the following:

```{r, comment=NA,warning=FALSE,message=FALSE}
#setup variables for the different projection systems
latlong <- "+init=epsg:4326" #WGS1984 / latitude & longitude
bng <- "+init=epsg:27700" #BNG, British National Grid
#create crime points as spatial data, they are currently in latlong
coords <- cbind(Longitude = crimes$Longitude, Latitude = 
                  crimes$Latitude)
crime.pts <- SpatialPointsDataFrame(coords, crimes[, -(5:6)], 
                                    proj4string = CRS(latlong))
```

This creates a ```SpatialPointsDataFrame``` object. This fifth line (starting `coords`) prepares the coordinates into a form that the ```SpatialPointsDataFrame``` can use. The ```SpatialPointsDataFrame``` function on the sixth line takes three arguments - the first is coordinates, created in the line above. The second argument is the data frame *minus* (i.e. not including) columns 5 and 6 - this is what ```-(5:6)``` indicates.  These columns provide all the non-geographical data from the data frame.  The third is the coordinate system that the data is currently in. The resulting object ```crime.pts``` is a spatial points geographical shape object,  whose points are each recorded crime in the data set you download. Try `head(crime.pts@data)` to see the attribute table, similar to earlier. 

We also need to reproject the data, from Latitude Longitude, to British National Grid:

```{r, comment=NA,warning=FALSE,message=FALSE}
#reproject to British National Grid, from Latitude Longitude
library(rgdal)
crime.pts <- spTransform(crime.pts, CRS(bng))
```

To see the geographical pattern of these crimes,  enter:
```{r}
plot(crime.pts,pch='.',col='darkred')
```

We can see the rough outline of the Merseyside Police area, as well as the path of the River Mersey. The option ```pch='.'``` tells R to map each point with a full-stop character - and to make the colour of these dark red.

You can also examine the kinds of different crimes recorded.
```{r, eval=FALSE,comment=NA,warning=FALSE,message=FALSE}
table(crime.pts$Crime.type)
```

##Point in Polygon

Having the point data is great, but in some ways it doesn't really tell us much. For example, we might be interested in where most crime takes place. However, at the moment all we can say is that most crime happens in the centre of the city, but that also tends to be where most people live. What we can do it look at how many crimes occur in each LSOA, and also work out the crime rate (i.e. crimes per ten-thousand people). 

To start with, we can overlay the LSOA we used earlier on the crimes. We can plot the crimes as before:

```{r, eval=FALSE}
plot(crime.pts,pch='.',col='darkred')
```

What we can also do is plot the LSOA, and if we add the parameter `add = TRUE` then R will plot the `LSOA` on top of the `crime.pts` layer. 

```{r}
plot(crime.pts,pch='.',col='darkred')
plot(LSOA, add = TRUE)
```

This doesn't really help much, as R sets the window to the first data set plotted. Try plotting the LSOA first, then the crimes layer. Remember to swap the `add = TRUE` parameter to the second plot command. 

This is still just two layers overlaid, but hopefully we can see what is going on now. A common GIS function is 'point-in-polygon' which will allow us (or at least the computer) to count how many crimes have been reported in each LSOA. 

```{r,message=FALSE,error=FALSE,warning=FALSE}
# This is another R package, allowing GIS overlay operations
library(rgeos) 
# This counts how many crimes occur in each LSOA
crime.count <- colSums(gContains(LSOA, crime.pts, byid = TRUE))
```

It is possible that R complains about the two spgeom (i.e. spatial geometries, or in English, spatial variables) having different projections. In this case, we know that both layers are in BNG, but `LSOA` doesn't have it's projection defined (try running `LSOA@proj4string` and compare that with `crime.pts@proj4string`). It's not unusual for shape files not to have their projection defined, and this can cause many problems. In this case, we can ignore the message, but if it happens in the future, it's good to check which projection & coordinate system the data are in.

Now we have the data, we can join it on to the spatial data, and draw a map. All the data is in the same order at the shape file, so we can just add it on to the end of the attribute table. 

```{r,message=FALSE,error=FALSE,eval=FALSE}
# Add a crime count column to the 'LSOA' SpatialPolygonsDataFrame
  LSOA@data$crime.count <- crime.count
# Now draw a choropleth map - using the same method as before
  var <- LSOA@data[,"crime.count"]
  breaks <- classIntervals(var, n = 6, style = "fisher")
  my_colours <- brewer.pal(6, "Greens")
  plot(LSOA, col = my_colours[findInterval(var, breaks$brks, all.inside = TRUE)],   
       axes = FALSE, border = rgb(0.8,0.8,0.8))
  legend(x = 330130, y = 385506.5, legend = leglabs(breaks$brks), fill = my_colours, bty = "n")
  title('Count of Crimes in Liverpool')
```

## Calculating the Crime Rate (optional exercise)

This just tells us how many crimes there were in each LSOA - nothing about the rate. Try calculating the rate of the crimes per 10,000 people and mapping these. You can calculate the rate by calculating (number of crimes / population)*10000. See the beginning of the handout for help on calculating values. Remember you will need to refer explicitally to the data frame (`LSOA@data`) each time you mention a column, i.e. `(LSOA@data$(fieldname) / LSOA@data$(fieldname)) * 10000`. 


## How many top crime areas in each Westminster Constituency? 

Let's say we want to find out how many of the top 10 crime LSOAs in Liverpool are in each of the 5 Westminster Constituencies that cover Liverpool (the constituencies can be downloaded from [www.nickbearman.me.uk/data/r/liverpool-westminster-const.zip](www.nickbearman.me.uk/data/r/liverpool-westminster-const.zip)). To do this, we need to work out the ten LSOAs with the highest crime count (or rate), and then see how many of them fall in the different Westminster Constituencies of Liverpool. We can reorder the LSOA data frame to find the top 10, and then we can use `gOverlaps` to count how many are in each constituency. 

```{r,message=FALSE,error=FALSE,eval=FALSE}
#reorder the data frame
  LSOA_reordered <- LSOA[order(LSOA@data$crime.count, decreasing = TRUE),]
```

Once the data are reordered, we can select out the top 10 using `LSOA_reordered[1:10,]`. The function `gContains(a,b)` will count the number of polygons in SpatialDataFrame `b` that are completly contained within SpatialDataframe `a` (i.e. have no bits sticking out). This is not the case for most of the LSOAs within the constituencies, so we need to use `gOverlaps` which counts both those that are completly within and those that overlap. 

See if you can work out the code required. If you're having trouble, try plotting the constituencies over the LSOA to begin with. You can plot the top 10 by using `plot(LSOA_reordered[1:10,], col = "red", add = TRUE)`. *(The code is available, but try not to look at it until you really have to!).* 

*The `rgeos` library contains a range of different spatial functions, including `gContains`, `gIntersects`, `gOverlaps`, `gRelate` and many others. These can be used to perform a spatial join, and other spatial analyses, but the exact function you will need to use depends on the type of data (point, line or polygon) and the type of join you are performing. The help files and a web search will provide useful information for this. There is also a function for buffers called `gBuffer`.*

*Merging shapefiles is also possible, but can be complex, depending on exaclty what you want to merge. Vignettes (short examples of code) are common with R libaries, and show how a particualar function works. For example, see http://cran.r-project.org/web/packages/maptools/vignettes/combine_maptools.pdf on Combining Spatial Data.*

## Writing Shape Files

We read shape files into R using `readOGR` and if you read the help file for this function (type `?readOGR`) you can see it talks about a `writeOGR` function, which we can use to export shape files. 

Having created our `crime.pts` SpatialPointsDataFrame, we can save this as a shapefile. This is a useful way of exporting data for use in ArcGIS, QGIS or other GIS programs. The code we need is:

```{r,eval=FALSE,message=FALSE,error=FALSE,warning=FALSE}
#write out crime.pts as a shapefile
writeOGR(obj=crime.pts, dsn=".", layer="crime", driver="ESRI Shapefile")

```

```{r,echo=FALSE,message=FALSE,error=FALSE,warning=FALSE}
#alternate code to above to overwrite shapefile layer, as specified at https://www.rdocumentation.org/packages/rgdal/versions/1.2-5/topics/writeOGR using overwrite_layer = TRUE, check_exists = TRUE
writeOGR(obj=crime.pts, dsn=".", layer="crime", driver="ESRI Shapefile", overwrite_layer = TRUE, check_exists = TRUE)

```

This code should create a new shapefile in your working directory called `crime`. The `writeOGR()` function uses the OGR drivers to load and save many different formats of spatial data. The help has some information (see also http://www.inside-r.org/packages/cran/rgdal/docs/writeOGR), and you can use `ogrDrivers()` to get a list of the different formats this covers (see also http://www.gdal.org/ogr_formats.html). An overview of the different options for reading and writing shapefiles in R at https://www.nceas.ucsb.edu/scicomp/usecases/ReadWriteESRIShapeFiles.

##'Heat-map' style map (optional exercise)

In this section we will look at creating a 'heat-map' style map, using crime data for Liverpool.

The next stage is to create the surface, based on the crime points. There are a number of different stages, which produce a number of different graphics. There are quite a few lines of code here, with relativly few comments, but this can be quite common in code vignettes that come with packages. 

<!-- version to run without producing maps -->
```{r, comment=NA,echo=FALSE,message=FALSE,warning=FALSE,error=FALSE}
#libraries
library(spatstat)
library(sp)
library(rgeos)
sSp <- as(SpatialPoints(crime.pts), "ppp")
Dens <- density(sSp, adjust = 0.2)
#class(Dens)
#plot(Dens)
#contour(density(sSp, adjust = 0.2), nlevels = 4)
Dsg <- as(Dens, "SpatialGridDataFrame")
Dim <- as.image.SpatialGridDataFrame(Dsg)
Dcl <- contourLines(Dim)
SLDF <- ContourLines2SLDF(Dcl)
proj4string(SLDF) <- proj4string(crime.pts)  # assign correct CRS - for future steps
```

<!-- not run, but used to show code -->
```{r, comment=NA,eval=FALSE}
#load required libraries
library(spatstat)
library(sp)
library(rgeos)
#create density map from crime.pts variable
sSp <- as(SpatialPoints(crime.pts), "ppp")
Dens <- density(sSp, adjust = 0.2)
class(Dens)
#plot density and customise
plot(Dens)
contour(density(sSp, adjust = 0.2), nlevels = 4)
#categorise density
Dsg <- as(Dens, "SpatialGridDataFrame")
Dim <- as.image.SpatialGridDataFrame(Dsg)
Dcl <- contourLines(Dim)
SLDF <- ContourLines2SLDF(Dcl)
proj4string(SLDF) <- proj4string(crime.pts)  # assign correct CRS - for future steps
#plot map
plot(SLDF, col = terrain.colors(8))
#overlay original crime point data
plot(crime.pts, pch = ".", col = "darkred", add = T)
```

You'll see that we have some additional data, particularly at the top right of the map. These are just contours with a value of 0, and can be removed.

```{r, comment=NA}
#remove the entries that are 0
SLDF <- SLDF[SLDF@data$level != 0,]
#and replot
plot(SLDF, col = terrain.colors(8))
plot(crime.pts, pch = ".", col = "darkred", add = T)
```

We can also highlight the area with the highest value in a particular colour.

```{r, comment=NA}
#add highlight for highest area
Polyclust <- gPolygonize(SLDF[5, ]) #adjust to alter threshold
gas <- gArea(Polyclust, byid = T)/10000
Polyclust <- SpatialPolygonsDataFrame(Polyclust, data = data.frame(gas), match.ID = F)
#Now summarise the data for each of the polygons.
cAg <- aggregate(crime.pts, by = Polyclust, FUN = length)
plot(SLDF, col = terrain.colors(8))
plot(cAg, col = "red", add = T)
```

You can change the number in the line `Polyclust <- gPolygonize(SLDF[5, ])` either up (to 8) or down (to 1) to alter which contour is filled in. 

Remember, the interpretation we can do on this data is quite limited, as we are looking at crimes, not crime rate. Therefore the map tells us a lot about population centres, but little about rates. 

You can see from the code that we are using data from January 2017 for the police crimes - try downloading more up to date data from www.police.uk and plotting that. If you need more help, there are some details on the Police.uk data in the second half of http://rpubs.com/nickbearman/gettingstartedwithr. 

You could also add an outline for Merseyside. The `plot()` command with the `add = TRUE` parameter will add the data to the existing plot. Try exploring the help, and looking at the previous code for details. 

```{r, comment=NA, results='hide'}
#download file
download.file("http://www.nickbearman.me.uk/data/r/merseyside_outline.zip",
              "merseyside_outline.zip")
#unzip file
unzip("merseyside_outline.zip")
#read in shapefile
mersey.outline <- readOGR(".","merseyside_outline")
```

This practical was written using R 3.4.0 (2017-04-21) and RStudio 1.0.143 by Nick Bearman (nick@clearmapping.co.uk). 

This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/deed.en. The latest version of the PDF is available from https://github.com/nickbearman/intro-r-spatial-analysis. This version was created on `r format(Sys.time(), '%d %B %Y')`. 

